---
title: "Full analysis - Yannick dataset"
author: "Jacques Marc-Antoine"
date: "11 ao√ªt 2017"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
source("../rscripts/package.R")
```


# Background, observations, questions

Each GF binds to a receptor and triggers a cascade that includes ERK plus others. But it is important for the cell to respond adequatly to the signal, e.g. proliferation vs differentiation. Each GF induces a different type of response when applied in a sustained fashion:

* EGF: transient activation of ERK, triggers proliferation. The higher ERK concentration, the sharper the peak of response gets, due to negative feedback that are proportionally activated.
* NGF: sustained activation of ERK, triggers differentiation. Behaviour remains indentical whatever the concentration.
* FGF: different behaviours depending on the concentration. Low concentrations induce a sustained response while higher concentrations induce a transient activation followed by an increasing/sustained response.

Clustering gives us ways to see these different behaviours, but we would like a way to put numbers on these differences in behaviour, maybe eventually to automatically detect them. 

Data look like:
```{r}
# Remove unnecessary columns for clarity
head(Yanni)
del.cols <- names(Yanni)[5:9]
Yanni[, (del.cols) := NULL]
Yanni
```

```{r warning=FALSE, cache=TRUE, fig.height=12, fig.width=21}
ggplot(Yanni, aes(y=Ratio, x=RealTime)) + geom_line(aes(group=Label)) + facet_wrap(~Condition) + scale_y_continuous(limits = c(1000, 1750)) + scale_x_continuous(limits = c(0,200)) + stat_summary(fun.y=mean, geom="line", colour = "blue", size = 1.5) + theme(text = element_text(size = 25))
```


# Clustering quality

Idea: Hclust, Kmeans... Use the clustering quality criteria (David-Bouldin, Dunn...) as a measure of distance between the clusters.

Here Kmeans/Kmedoids seem appropriate, we guess that the number of clusters should be no more than let's say 10. 


## Data normalization

Prior to clustering, normalize data since the computations of distances between TS always relie on some kind of Euclidean distance.  We use the fold-change "Korean way", which normalizes in a per trajectory fashion. 

* Time range used for normalization: 0-36
* Trim x-axis (time): 0-200

```{r}
# Normalization
Yanni <- myNorm(in.dt = Yanni, in.meas.col = "Ratio", in.rt.min = 0, in.rt.max = 36, in.by.cols = c("Condition", "Label"), in.type = "fold.change")
# X trimming
Yanni <- Yanni[RealTime <= 200]
```


## Smoothing

Erase small variations in the signal that are not relevant for distance computation, smooth out high frequency variations (noise).
```{r fig.height=12, fig.width=21, message=FALSE}
Yanni[, Ratio.norm.smooth := rollex(Ratio.norm, k = 5), by = c("Condition", "Label")]
Yanni

# Plot first trajetory
par(mfrow=c(3,1))
plot(Yanni[Condition=="E-0.25" & Label=="12_0002", Ratio], type = "b", ylab = "value", cex.main=3, cex.axis=2.5, main = "Raw")
plot(Yanni[Condition=="E-0.25" & Label=="12_0002", Ratio.norm], type = "b", ylab = "value", cex.main=3, cex.axis=2.5, main = "Normalized")
plot(Yanni[Condition=="E-0.25" & Label=="12_0002", Ratio.norm.smooth], type = "b", ylab = "value", cex.main=3, cex.axis=2.5, main = "Normalized and smoothed")
```


## Clustering indices

Will use a method based on DTW, much more tailored to TS than classical Euclidean distance. In addition we use partionning around medoids instead of Kmeans, these are basically the same methods except that medoids are much less sensitive to outliers. Instead of picking the mean of a cluster as a center, medoids pick the data point that is the closest to the middle of the cluster.

Though the method is dependant on the initialization, changing the seed doesn't affect the clustering that much.

```{r}
CastCluster <- function(data, time.col, condition.col, label.col, measure.col, k.clust, na.fill, plot = T, return.quality = T, ...){
  # Cast to wide, cluster and get quality indices
  require(dtwclust)
  temp <- myCast(data, time.col, condition.col, label.col, measure.col, na.fill)
  
  # Make clustering, and get quality indexes
  clust <- tsclust(temp$casted.matrix, type = "partitional", k = k.clust, distance = "dtw_basic", centroid = "pam", seed = 42, ...)
  names(clust) <- paste0("k_", k.clust)
  quality <- sapply(clust, cvi, type = "internal")
  
  # Add a column with the clusters to the casted table
  cluster.table <- temp$casted
  for(k in 1:length(clust)){
    cluster.table <- cbind(clust[[k]]@cluster, cluster.table)
    colnames(cluster.table)[1] <- names(clust)[k]
  }
  
  # Plot
  if(plot){
    require(ggplot2)
    mquality <- melt(quality)
    names(mquality) <- c("Stat", "Nb.Clust", "value")
    plot(ggplot(mquality, aes(x=Nb.Clust, y = value)) + geom_col(aes(group = Nb.Clust, fill = Nb.Clust)) + facet_grid(Stat ~ ., scales = "free_y"))
  }
  
  # Output
  if(return.quality) return(list(cluster = clust, table = cluster.table, quality = quality))
  else return(list(out = clust, table = cluster.table))
}


myCast <- function(data, time.col, condition.col, label.col, measure.col, na.fill){
  # Only cast to wide matrix
  temp <- copy(data)
  # dcast can change the order of the rows depending on the orde rin which the keyed columns are passed, keep the casted table in addition to the matrix to make the link afterwards
  temp <- dcast(temp, get(condition.col) + get(label.col) ~ get(time.col), value.var = measure.col)
  temp2 <- as.matrix(temp[, c(-1, -2)]) # remove 2 first columns with labels
  temp2[which(is.na(temp2))] <- na.fill
  return(list(casted = temp, casted.matrix = temp2))
}


plot_cluster <- function(data, id.vars.col, cluster.col, type){
  # Plot clusters directly from output$table of CastCluster
  # id.vars.col: given in indices (include ALL clustering columns)
  # cluster.col: name of the column with clustering to plot
  library(ggplot2)
  ids <- colnames(data)[id.vars.col]
  melted <- melt(data, id.vars = ids)
  if(type=="trajectory"){
    ggplot(melted, aes(x = as.numeric(variable), y = value)) + geom_line(aes(group = label.col)) +
    facet_wrap(as.formula(paste("~",cluster.col))) + stat_summary(fun.y=mean, geom="line", colour = "blue", size = 1.5) + xlab("Time")
  } else if(type=="composition"){
    melted[, c(cluster.col):=as.factor(get(cluster.col))]
    ggplot(melted, aes_string(cluster.col)) + geom_bar(aes(fill=condition.col, y = (..count..)/sum(..count..)))
  }
}
```

```{r}
EGF <- Yanni[Condition %in% c("E-0.25", "E-2.5", "E-25", "E-250")]
NGF <- Yanni[Condition %in% c("N-0.25", "N-2.5", "N-25", "N-250")]
FGF <- Yanni[Condition %in% c("F-0.25", "F-2.5", "F-25", "F-250")]

cond <- "Condition"
lab <-  "Label"
tim <- "RealTime"
k.clus <- 2L:8L
na.f.raw <- 1200
na.f.nor <- 1.1
```

Clustering quality is evaluated with the following indices (see ?dtwclust::cvi ; https://pdfs.semanticscholar.org/a522/fb4646ad19fe88893b90e6fbc1faa1470976.pdf):

* CH: Calinski-Harabasz index, to MAXIMIZE
* COP: COP index, to MINIMIZE
* D: Dunn index, to MAXIMIZE
* DB: Davis-Bouldin, to MINIMIZE
* DBstar: modified Davis-Bouldin, to MINIMIZE
* SF: Score Function, to MAXIMIZE
* Sil: Silhouette index, to MAXIMIZE


Results with these indices is very often unclear and contradictory, inspecting the cluster composition can help. In a setting where one concentration gives rise to an unique response, we would expect a good clustering to contain clusters with at least 25% of the observations (since there is 4 concentrations) that are enriched/depleted in at least one concentration.


### EGF

```{r cache=T, message=F}
# With unormalized and unsmoothed data
clust_EGF_1 <- CastCluster(EGF, time.col = tim, condition.col = cond,  label.col = lab, k.clust = k.clus, na.fill = na.f.raw, plot = F, measure.col = "Ratio")
# With normalized and unsmoothed data
clust_EGF_2 <- CastCluster(EGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm")
# With normalized and smoothed data
clust_EGF_3 <- CastCluster(EGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm.smooth")
```

Advised number of clusters:
```{r cache=T}
raw <- c(apply(clust_EGF_1$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_EGF_1$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
norm <- c(apply(clust_EGF_2$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_EGF_2$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
smooth <- c(apply(clust_EGF_3$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_EGF_3$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
rbind(raw, norm, smooth)
```


```{r warning=F, echo=F, fig.width=17, fig.height=8, cache=T}
q1 <- melt(as.data.table(clust_EGF_1$quality))
q2 <- melt(as.data.table(clust_EGF_2$quality))
q3 <- melt(as.data.table(clust_EGF_3$quality))

q1$Ratio <- "Raw"
q2$Ratio <- "Norm"
q3$Ratio <- "NormSmooth"
temp <- rbind(q1, q2, q3)
temp$index <- rep(rownames(clust_EGF_1$quality), 21)
ggplot(temp, aes(x=variable, y = value)) + geom_col(aes(fill = as.factor(Ratio)), position = "dodge") + facet_grid(index ~ ., scales = "free_y") + theme(text = element_text(size = 20)) + xlab("Number Cluster")
```

```{r cache=T}
plot_cluster(clust_EGF_3$table, id.vars.col = 1:9, cluster.col = "k_2", type = "trajectory")
```


```{r fig.width=21, fig.height=12, cache=T}
# Cluster composition with Raw data
library(gridExtra)
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_EGF_1$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)

# With normalized smoothed data
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_EGF_3$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)
```

Each cluster systematically contains the 4 concentrations, very low separabilty --> 1 type of response whatever the concentration.

### NGF

```{r cache=T, message=F}
clust_NGF_1 <- CastCluster(NGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.raw, plot = F, measure.col = "Ratio")
clust_NGF_2 <- CastCluster(NGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm")
clust_NGF_3 <- CastCluster(NGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm.smooth")
```

Advised number of clusters:
```{r cache=T}
raw <- c(apply(clust_NGF_1$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_NGF_1$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
norm <- c(apply(clust_NGF_2$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_NGF_2$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
smooth <- c(apply(clust_NGF_3$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_NGF_3$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
rbind(raw, norm, smooth)
```

```{r warning=F, echo=F, fig.width=17, fig.height=8, cache=T}
q1 <- melt(as.data.table(clust_NGF_1$quality))
q2 <- melt(as.data.table(clust_NGF_2$quality))
q3 <- melt(as.data.table(clust_NGF_3$quality))

q1$Ratio <- "Raw"
q2$Ratio <- "Norm"
q3$Ratio <- "NormSmooth"
temp <- rbind(q1, q2, q3)
temp$index <- rep(rownames(clust_NGF_1$quality), 21)
ggplot(temp, aes(x=variable, y = value)) + geom_col(aes(fill = as.factor(Ratio)), position = "dodge") + facet_grid(index ~ ., scales = "free_y") + theme(text = element_text(size = 20)) + xlab("Number Cluster")
```

```{r cache=T}
plot_cluster(clust_NGF_3$table, id.vars.col = 1:9, cluster.col = "k_2", type = "trajectory")
```

```{r fig.width=21, fig.height=12, cache=T}
# Cluster composition with Raw data
library(gridExtra)
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_NGF_1$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)

# With normalized smoothed data
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_NGF_3$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)
```

Each cluster systematically contains the 4 concentrations, very low separabilty --> 1 type of response whatever the concentration.

### FGF

```{r cache=T, message=F}
clust_FGF_1 <- CastCluster(FGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.raw, plot = F, measure.col = "Ratio")
clust_FGF_2 <- CastCluster(FGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm")
clust_FGF_3 <- CastCluster(FGF, time.col = tim, condition.col = cond, label.col = lab, k.clust = k.clus, na.fill = na.f.nor, plot = F, measure.col = "Ratio.norm.smooth")
```

Advised number of clusters:
```{r cache=T}
raw <- c(apply(clust_FGF_1$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_FGF_1$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
norm <- c(apply(clust_FGF_2$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_FGF_2$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
smooth <- c(apply(clust_FGF_3$quality[c("CH","D","SF","Sil"),], 1, which.max) + 1, apply(clust_FGF_3$quality[c("COP","DB","DBstar"),], 1, which.min) + 1)
rbind(raw, norm, smooth)
```

```{r warning=F, echo=F, fig.width=17, fig.height=8, cache=T}
q1 <- melt(as.data.table(clust_FGF_1$quality))
q2 <- melt(as.data.table(clust_FGF_2$quality))
q3 <- melt(as.data.table(clust_FGF_3$quality))

q1$Ratio <- "Raw"
q2$Ratio <- "Norm"
q3$Ratio <- "NormSmooth"
temp <- rbind(q1, q2, q3)
temp$index <- rep(rownames(clust_FGF_1$quality), 21)
ggplot(temp, aes(x=variable, y = value)) + geom_col(aes(fill = as.factor(Ratio)), position = "dodge") + facet_grid(index ~ ., scales = "free_y") + theme(text = element_text(size = 20)) + xlab("Number Cluster")
```

```{r cache=T}
plot_cluster(clust_FGF_3$table, id.vars.col = 1:9, cluster.col = "k_2", type = "trajectory")
```

```{r fig.width=21, fig.height=12, cache=T}
# Cluster composition with Raw data
library(gridExtra)
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_FGF_1$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)

# With normalized smoothed data
for(i in k.clus){
  name <- paste0("k_",i)
  base::assign(paste0("q",i), plot_cluster(clust_FGF_3$table, id.vars.col = 1:9, cluster.col = name, type="composition"))
}
grid.arrange(q2,q3,q4,q5,q6,q7,q8, ncol =4, nrow=2)
```

Separability seems better than for EGF and NGF, notably concentration 0.25 and 250 tend to be taken apart.

## Choose number of clusters with the gap statistics

The gap statistics is an heuristic way of estimating the number of clusters. It compares the variance explained by our clustering to the expected explained variance if there was no cluster but just noise in that specific space region. It is very computational costly but has the major advantage to also indicate whether clustering is even relevant in the first place (i.e. k=1 should be kept). See: https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/

```{r cache = TRUE, eval=F}
modtsclust <- function(data, k, type, centroid){
  # Interface function compatible with clusGap, returns only the result of the clustering
  require(dtwclust)
  temp <- tsclust(data, k = k, type = type, centroid = centroid)
  return(list(cluster = temp@cluster))
}
kmax = 6L

require(cluster)
temp <- copy(EGF)
temp <- dcast(temp, Label ~ RealTime, value.var = "Ratio.norm.smooth")
temp <- as.matrix(temp[, -1]) # remove first columns with labels
temp[which(is.na(temp))] <- 1.1
cgap_EGF_smooth <- clusGap(temp, FUN=modtsclust, K.max=kmax, B=50, centroid = "pam", type = "partitional")

temp <- copy(NGF)
temp <- dcast(temp, Label ~ RealTime, value.var = "Ratio")
temp <- as.matrix(temp[, -1]) # remove first columns with labels
temp[which(is.na(temp))] <- 1.1
cgap_NGF <- clusGap(temp, FUN=modtsclust, K.max=kmax, B=100, centroid = "pam", type = "partitional")

temp <- copy(FGF)
temp <- dcast(temp, Label ~ RealTime, value.var = "Ratio.norm.smooth")
temp <- as.matrix(temp[, -1]) # remove first columns with labels
temp[which(is.na(temp))] <- 1.1
cgap_FGF_smooth <- clusGap(temp, FUN=modtsclust, K.max=kmax, B=100, centroid = "pam", type = "partitional")


for(k in 1:(kmax-1)) {
    if(cgap_FGF_smooth$Tab[k,3]>cgap_FGF_smooth$Tab[(k+1),3]-cgap_FGF_smooth$Tab[(k+1),4]) {print(k)}; 
    break;
}
```


# Separability of snapshots between concentrations of the same GF

Idea: For a given GF, at each time point and for each concentration get the signal distribution, check if they overlap well between different concentrations. Overlap can be measured by KS statistics, Bhattacharryya index, Kullback‚ÄìLeibler...

However this implicitly assumes that there's no subpopulation within a given condition (GF + concentration).

## Raw values

```{r warning=F}
sep.meas.along.time <- function(data1, data2, time.col, measure.col){
  timev <- unique(data1[, get(time.col)])
  if(!(identical(unique(data2[, get(time.col)]), timev))) stop("Time vectors must be identical between the two data")
  out <- separability.measures(data1[get(time.col)==timev[1], get(measure.col)], data2[get(time.col)==timev[1], get(measure.col)])
  for(t in timev[2:length(timev)]){
    out <- rbind(out, separability.measures(data1[RealTime==t, get(measure.col)], data2[RealTime==t, get(measure.col)]))
  }
  out <- cbind(timev, out)
  return(out)
}

library(plyr)

# Get all pairs of conditions
conditions <- combn(as.character(unique(Yanni[,Condition])), m = 2)
conditions <- conditions[,c(1:3,12,13,22, 39:41,46,47,52, 61:66)]

# Compute separabilities of conditions at each time point
sep.meas.raw <- apply(conditions, 2, function(x) sep.meas.along.time(Yanni[Condition==x[1]],  Yanni[Condition==x[2]], "RealTime", "Ratio" ))
names(sep.meas.raw) <- apply(conditions, 2, function(x) paste(x[1], x[2], sep = ","))

# Go to data table
for(i in 1:length(sep.meas.raw)){
  temp <- unlist(strsplit(names(sep.meas.raw)[i], ","))
  sep.meas.raw[[i]]$Cond1 <- temp[1]
  sep.meas.raw[[i]]$Cond2 <- temp[2]
}

sep.meas.raw <- as.data.table(rbind.fill(sep.meas.raw))
sep.meas.raw[, c("Cond1", "Cond2") := list(as.factor(Cond1), as.factor(Cond2))]
```


```{r fig.width=21, fig.height=12}
ggplot(sep.meas.raw, aes(x= timev, y = jm)) + geom_line() + facet_wrap(~Cond1 + Cond2) + theme(text=element_text(size=25)) + ylab("Jeffries-Matusita [0, sqrt(2)]") + geom_vline(xintercept=40, colour="blue", linetype="longdash")
```

Can sum up these curves with area under curve:
```{r}
max.val <- sqrt(2) * 101
auc <- sep.meas.raw[, .(auc = sum(jm, na.rm = T)/max.val), by = c("Cond1", "Cond2")] # a few NAs, slight bias in the values
auc
```

The indicator seems to show the differences in behaviour really well: NGF curves are completely flat, indicating that there is few deviations between 2 concentrations. As for NGF and EGF 2 about clusters appear between bahaviours at low versus behaviours at high concentrations.

The peaks observed aorund 40min, can either come from differences in excitation level or from a shift between series, this needs to be checked. If the prior was true, should perform a multiple alignment within each concentration prior to compute the distances.

## Normalized valued

```{r warning=F}
sep.meas.norm <- apply(conditions, 2, function(x) sep.meas.along.time(Yanni[Condition==x[1]],  Yanni[Condition==x[2]], "RealTime", "Ratio.norm" ))
names(sep.meas.norm) <- apply(conditions, 2, function(x) paste(x[1], x[2], sep = ","))

# Go to data table
for(i in 1:length(sep.meas.norm)){
  temp <- unlist(strsplit(names(sep.meas.norm)[i], ","))
  sep.meas.norm[[i]]$Cond1 <- temp[1]
  sep.meas.norm[[i]]$Cond2 <- temp[2]
}

sep.meas.norm <- as.data.table(rbind.fill(sep.meas.norm))
sep.meas.norm[, c("Cond1", "Cond2") := list(as.factor(Cond1), as.factor(Cond2))]
```


```{r fig.width=21, fig.height=12}
ggplot(sep.meas.norm, aes(x= timev, y = jm)) + geom_line() + facet_wrap(~Cond1 + Cond2) + theme(text=element_text(size=25)) + ylab("Jeffries-Matusita [0, sqrt(2)]") + geom_vline(xintercept=40, colour="blue", linetype="longdash")
```

```{r}
max.val <- sqrt(2) * 101
auc <- sep.meas.norm[, .(auc = sum(jm, na.rm = T)/max.val), by = c("Cond1", "Cond2")] # a few NAs, slight bias in the values
auc
```

Seems that the normalization makes no good to the metric by amplifying small differences (due to shift)?

## Normalized and smoothed

```{r}
sep.meas.smooth <- apply(conditions, 2, function(x) sep.meas.along.time(Yanni[Condition==x[1]],  Yanni[Condition==x[2]], "RealTime", "Ratio.norm.smooth" ))
names(sep.meas.smooth) <- apply(conditions, 2, function(x) paste(x[1], x[2], sep = ","))

# Go to data table
for(i in 1:length(sep.meas.smooth)){
  temp <- unlist(strsplit(names(sep.meas.smooth)[i], ","))
  sep.meas.smooth[[i]]$Cond1 <- temp[1]
  sep.meas.smooth[[i]]$Cond2 <- temp[2]
}

sep.meas.smooth <- as.data.table(rbind.fill(sep.meas.smooth))
sep.meas.smooth[, c("Cond1", "Cond2") := list(as.factor(Cond1), as.factor(Cond2))]
```


```{r fig.width=21, fig.height=12}
ggplot(sep.meas.smooth, aes(x= timev, y = jm)) + geom_line() + facet_wrap(~Cond1 + Cond2) + theme(text=element_text(size=25)) + ylab("Jeffries-Matusita [0, sqrt(2)]") + geom_vline(xintercept=40, colour="blue", linetype="longdash")
```

```{r}
max.val <- sqrt(2) * 101
auc <- sep.meas.smooth[, .(auc = sum(jm, na.rm = T)/max.val), by = c("Cond1", "Cond2")] # a few NAs, slight bias in the values
auc
```

Same as for normalization, probably better to stick to raw data if possible.

## To go further: bootstraps and permutation tests

### Permutation tests

Tells us if the observed area under the curve is really related to a difference between the two treatments or if it is spurious. If there is n trajectories recorded in condition 1 and m in condition 2, the permutation test is performed like so:

* Randomly assign n trajectories to group 1
* Assign the ret to group 2
* Compute the auc
* Repeat

We then look if the observed auc is likely in this empirical distribution.

```{r}
one.permutation.auc <- function(x, y, metric){
  n <- nrow(x)
  m <- nrow(y)
  temp <- rbind(x, y)
  samp.traj <- sample(1:nrow(temp), size = n, replace = FALSE)
  x.resamp <- temp[samp.traj, ]
  y.resamp <- temp[setdiff(1:nrow(temp), samp.traj), ]

  seps <- sapply(1:ncol(x), function(j) separability.measures(x.resamp[, j], y.resamp[, j]))
  return(sum(unlist(seps[metric, ])))
}

permutation.auc <- function(x, y, n, metric = "jm"){
  # x,y: two matrices representing time series, row: trajectory; col: time
  # n: number of permutations
  # metric: one of "jm", "bh", "div", "tdiv", "ks"
  if(ncol(x) != ncol(y)) stop("x and y must have same number of columns")
  return(replicate(n, one.permutation.auc(x,y,metric)))
}
```

```{r warning=F}
a <- myCast(FGF[Condition=="F-0.25"], "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
b <- myCast(FGF[Condition=="F-250"],  "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
temp <- permutation.auc(a, b, 500)
hist(temp/max.val, freq=F)
```

### Bootstraps

#### Bootstraps per column (i.e. resample time points)

In order to know whether these area under curves are robust estimates, could perform bootstraps the way it is performed in phylogenetics:

* Initialize an empty matrix with dimensions of the one representing the time series along time.
* From the initial matrix representing the trajectories, select a column at random (i.e. one time point)
* Add this column to the empty matrix

Because we deal here with comparison of 2 populations we can modify this to:

* Initialize two empty matrices, one for each population
* Pick a random column index (or a time)
* Extend both matrices with the column of its corresponding population.

This implies that matrices are of same length and defined at the same times.

```{r}
one.bootstrap.auc.percol <- function(x, y, metric){
  samp.col <- sample(1:ncol(x), size = ncol(x), replace = TRUE)
  x.resamp <- x[, samp.col]
  y.resamp <- y[, samp.col]
  seps <- sapply(1:ncol(x), function(j) separability.measures(x.resamp[, j], y.resamp[, j]))
  return(sum(unlist(seps[metric, ])))
}

bootstrap.auc.percol <- function(x, y, B, metric = "jm"){
  # x,y: two matrices representing time series, row: trajectory; col: time
  # B: number of boostraps
  # metric: one of "jm", "bh", "div", "tdiv", "ks"
  if(ncol(x) != ncol(y)) stop("x and y must have same number of columns")
  return(replicate(B, one.bootstrap.auc.percol(x,y,metric)))
}
```


```{r warning=F}
a <- myCast(FGF[Condition=="F-0.25"], "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
b <- myCast(FGF[Condition=="F-250"],  "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
temp <- bootstrap.auc.percol(a, b, 500)
hist(temp/max.val, freq=F)

# Standard Error of the estimator
summary(temp/max.val)
sd(temp/max.val)
```

We see that the average of the distribution corresponds to the observed value.


#### Bootstraps per row (i.e. resample trajectories)

A lack of robustness here could potentially show the presence of subpopulations, that are missed when simply sum up to AUC.

```{r}
one.bootstrap.auc.perrow <- function(x, y, metric){
  samp.rowx <- sample(1:nrow(x), size = nrow(x), replace = TRUE)
  samp.rowy <- sample(1:nrow(y), size = nrow(y), replace = TRUE)
  x.resamp <- x[samp.rowx, ]
  y.resamp <- y[samp.rowy, ]
  seps <- sapply(1:ncol(x), function(j) separability.measures(x.resamp[, j], y.resamp[, j]))
  return(sum(unlist(seps[metric, ])))
}

bootstrap.auc.perrow <- function(x, y, B, metric = "jm"){
  # x,y: two matrices representing time series, row: trajectory; col: time
  # B: number of boostraps
  # metric: one of "jm", "bh", "div", "tdiv", "ks"
  if(ncol(x) != ncol(y)) stop("x and y must have same number of columns")
  return(replicate(B, one.bootstrap.auc.percol(x,y,metric)))
}
```

```{r warning=F}
a <- myCast(EGF[Condition=="E-0.25"], "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
b <- myCast(EGF[Condition=="E-250"],  "RealTime", "Condition", "Label", "Ratio", 1100)$casted.matrix
temp <- bootstrap.auc.perrow(a, b, 500)
hist(temp/max.val)
```


# Cross-Correlations/Coherence between concentrations of the same GF

Idea: For each GF, compare each pair of trajectories that do not belong to the same concentrations. This comparison can be done by cross-correlation/overlap of clipping trajectories, simple correlations... Cross-correlations provides a way to align the signals as best as we can be shifting this is cool to get rid of differences that would come from experimental conditions like we've been waiting 5 more minutes to inject the GF.

Could also check coherence (cross-correlation based on spectral densities).


# Information theory approach: entropy of heatmap / signal

Idea: Quantize the signals and check how complex they are. Pool all concentrations of a single GF together and see how complex the resulting image is. We can quantize the signals and compute the entropy, or do it directly from a heatmap (quantization is then performed by color coding).    



